{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MFnuFm6bnQo"
   },
   "source": [
    "# Classification Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_ZijPcUPYDks"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,models\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fPshfwJ8Ys17"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (320, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dqVYBxwyYt03"
   },
   "outputs": [],
   "source": [
    "dataset = \"./DeepFire Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0aCpjE9BYvAS"
   },
   "outputs": [],
   "source": [
    "image_dir = Path(dataset)\n",
    "\n",
    "# Get filepaths and labels\n",
    "filepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png'))\n",
    "\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "# Concatenate filepaths and labels\n",
    "image_df = pd.concat([filepaths, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOH8TnBSYwQU",
    "outputId": "7ef7bfdd-db0d-46b0-c62d-e89b62ff06c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(image_dir.glob(r'**/*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YvwwRHjaamyi"
   },
   "outputs": [],
   "source": [
    "# Separate in train and test data\n",
    "train_df, test_df = train_test_split(image_df, test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IvrRbJXwaoOj"
   },
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9q-SRO2apwH",
    "outputId": "dcc1cf62-d617-407a-fed9-b6a6e20d68bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 931 validated image filenames belonging to 2 classes.\n",
      "Found 570 validated image filenames belonging to 2 classes.\n",
      "Found 399 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Split the data into three categories.\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qEL-iQ2IarZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 13:43:30.881550: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-07 13:43:30.882184: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Resize the Layer\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(224,224),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vTEUg9AZasqE"
   },
   "outputs": [],
   "source": [
    "# Load the Pre-Trained MobileNetV2 Model\n",
    "pretrained_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")\n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vyofQi2Yav9P"
   },
   "outputs": [],
   "source": [
    "# Create a Checkpoint Callback\n",
    "checkpoint_path = \"fires_classification_model_checkpoint\"\n",
    "checkpoint_callback = ModelCheckpoint(checkpoint_path,\n",
    "                                      save_weights_only=True,\n",
    "                                      monitor=\"val_accuracy\",\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mQCGnz0oaxEo"
   },
   "outputs": [],
   "source": [
    "# Setup EarlyStopping callback to stop training if the model's val_loss doesn't improve for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                               patience=5,\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DnqEzkA9ax8t"
   },
   "outputs": [],
   "source": [
    "# Add a new Fully Connected layer for binary classification\n",
    "inputs = pretrained_model.input\n",
    "\n",
    "x = pretrained_model.output\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Add a Fully Connected layer for binary classification (Fire or Non-Fire)\n",
    "outputs = Dense(2, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44H5kzHlg9nO",
    "outputId": "33639569-3d7c-407f-c85d-b16d1c55c672"
   },
   "outputs": [],
   "source": [
    "# model.save(\"mobilenetv2.keras\")\n",
    "import keras\n",
    "model = keras.models.load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhtBuF6ia03i",
    "outputId": "6082ac1e-ab72-41c8-9c48-3831e35b282e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 13:43:35.546003: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-04-07 13:43:35.790648: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test Loss: 0.03798\n",
      "Test Accuracy: 98.77%\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_images, verbose=0)\n",
    "\n",
    "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ4eoSNUbtT5"
   },
   "source": [
    "# Building The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rRxMLx42cBJT"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os, re, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from functools import reduce\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WyxhUPDxcCbi"
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Xp5WirmycDTb"
   },
   "outputs": [],
   "source": [
    "def get_image_feature(image):\n",
    "    temp = image # divide by 255 to get in fraction\n",
    "    mn = temp.sum(axis=0).sum(axis=0)/(temp.shape[0]*temp.shape[1])\n",
    "    return mn/np.linalg.norm(mn, ord=None) # taking 2nd norm to scale vector\n",
    "\n",
    "def get_image(name, folder, color_space='rgb'):\n",
    "    filepath = os.path.join(folder, name)\n",
    "    # img = Image.open(filepath)\n",
    "    # read imread as rgb\n",
    "    img = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
    "    # convert to rgb color space\n",
    "    if color_space == 'rgb':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    elif color_space == 'gray':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # resize image to 254x254\n",
    "    img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    # convert to rgb color space\n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "def get_images(directoryName, color_space='rgb'):\n",
    "    directory = os.fsencode(directoryName)\n",
    "    images = []\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "            image = get_image(filename, directoryName, color_space)\n",
    "            images.append(image)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lRa2zd_7cEhG"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def low_light(image):\n",
    "    # Convert to HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Apply histogram equalization to V channel\n",
    "    # hsv[:, :, 2] = cv2.equalizeHist(hsv[:, :, 2])\n",
    "\n",
    "    # # Apply median filter to V channel\n",
    "    # hsv[:, :, 2] = cv2.medianBlur(hsv[:, :, 2], 5)\n",
    "\n",
    "    # Convert back to RGB\n",
    "    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return img\n",
    "\n",
    "def apply_hsv_rule(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    # Convert to HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Split channels\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    # transform hsv to 0-1 range\n",
    "    h = h/255\n",
    "    s = s/255\n",
    "    v = v/255\n",
    "\n",
    "    # get rgb component\n",
    "    r, g, b = cv2.split(image)\n",
    "\n",
    "    # Define rules\n",
    "    mask = (h>=0.13) & (h<0.46) & (s>=0.1) & (s<=0.34) & (v>=0.96) & (v<=1) & (r>=g) & (g>b) & (r>180) & (g>130) & (b<120)\n",
    "\n",
    "    # Apply mask\n",
    "    img[mask] = 255\n",
    "\n",
    "    img[~mask] = 0\n",
    "\n",
    "    return img\n",
    "\n",
    "def apply_ycbcr_rule(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    # Convert to YCbCr\n",
    "    ycbcr = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "    # Split channels\n",
    "    y, cb, cr = cv2.split(ycbcr)\n",
    "\n",
    "    # get rgb component\n",
    "    r, g, b = cv2.split(image)\n",
    "\n",
    "    # Define rules\n",
    "    mask = (y>cr) & (cr>=cb) & (y>=180) & (y<210) & (cb>=80) & (cb<=120) & (cr>=80) & (cr<=139) & (r>=g) & (g>b) & (r>190) & (g>110) & (b<180)\n",
    "\n",
    "    # Apply mask\n",
    "    img[mask] = 255\n",
    "\n",
    "    img[~mask] = 0\n",
    "\n",
    "    return img\n",
    "\n",
    "def apply_rgb_rule(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    # Split channels\n",
    "    r, g, b = cv2.split(image)\n",
    "\n",
    "    # Define rules\n",
    "    mask = (r>g) & (g>b) & (r>200) & (g>130) & (b<120)\n",
    "\n",
    "    # Apply mask\n",
    "    img[mask] = 255\n",
    "\n",
    "    img[~mask] = 0\n",
    "\n",
    "    return img\n",
    "\n",
    "def segment_images(images):\n",
    "    result = []\n",
    "    for image in images:\n",
    "        cpy_img = np.copy(image)\n",
    "\n",
    "        # apply low light\n",
    "        cpy_img = low_light(cpy_img)\n",
    "\n",
    "        # apply hsv rules\n",
    "        ycbcr_img = apply_ycbcr_rule(cpy_img)\n",
    "        hsv_img = apply_hsv_rule(cpy_img)\n",
    "        rgb_img = apply_rgb_rule(cpy_img)\n",
    "\n",
    "        # combine all masks using or operator\n",
    "        ycbcr_img = np.logical_or(ycbcr_img, hsv_img)\n",
    "        ycbcr_img = np.logical_or(ycbcr_img, rgb_img)\n",
    "\n",
    "        # mask original image\n",
    "        cpy_img[~ycbcr_img] = 0\n",
    "        cpy_img[ycbcr_img] = 255\n",
    "\n",
    "        result.append(cpy_img)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "def segment_image(image):\n",
    "    cpy_img = np.copy(image)\n",
    "\n",
    "    # apply low light\n",
    "    cpy_img = low_light(cpy_img)\n",
    "\n",
    "    # apply hsv rules\n",
    "    ycbcr_img = apply_ycbcr_rule(cpy_img)\n",
    "    hsv_img = apply_hsv_rule(cpy_img)\n",
    "    rgb_img = apply_rgb_rule(cpy_img)\n",
    "\n",
    "    # combine all masks using or operator\n",
    "    ycbcr_img = np.logical_or(ycbcr_img, hsv_img)\n",
    "    ycbcr_img = np.logical_or(ycbcr_img, rgb_img)\n",
    "\n",
    "    # mask original image\n",
    "    cpy_img[~ycbcr_img] = 0\n",
    "\n",
    "    return cpy_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eJ8AsGAOcF1F"
   },
   "outputs": [],
   "source": [
    "def morphological_operations(image, kernel_size, iterations):\n",
    "    # Create a kernel for morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n",
    "\n",
    "    # Perform closing operation\n",
    "    closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel, iterations=iterations)\n",
    "\n",
    "    # Perform dilation operation\n",
    "    dilated_image = cv2.dilate(closed_image, kernel, iterations=iterations)\n",
    "\n",
    "    # close holes\n",
    "    # dilated_image = cv2.morphologyEx(dilated_image, cv2.MORPH_CLOSE, kernel, iterations=iterations)\n",
    "\n",
    "\n",
    "    return dilated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yxJqUmKEfpmu"
   },
   "outputs": [],
   "source": [
    "def get_prediction(image):\n",
    "    masked = segment_image(image)\n",
    "\n",
    "    masked = cv2.cvtColor(masked, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    masked = morphological_operations(masked, (5, 5), 4)\n",
    "\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSLaL8TSojg1",
    "outputId": "504d9265-1a96-45ef-e122-32b387ce6662"
   },
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"./FireSegmentationDataset/Video01/Video01_Frame/Video01_Frame001.jpg\")\n",
    "test_image_reshaped = np.resize(test_image,(1, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_veBVe22jKW8"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    pred = model.predict(image)\n",
    "    mask = get_prediction(image)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpALKRQKj0_F",
    "outputId": "bb866424-58aa-4761-b72b-eb19a3d5ddd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "CPU times: user 72.3 ms, sys: 22.5 ms, total: 94.8 ms\n",
      "Wall time: 87.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = model.predict(test_image_reshaped)\n",
    "mask = get_prediction(test_image_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv8_1 = YOLO('./fire-segmentation-master/models/bestv8_1.pt')\n",
    "modelv8_2 = YOLO('./fire-segmentation-master/models/bestv8_2.pt')\n",
    "\n",
    "modelv8 = [modelv8_1, modelv8_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_light(image):\n",
    "    # Convert to HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Apply histogram equalization to V channel\n",
    "    hsv[:, :, 2] = cv2.equalizeHist(hsv[:, :, 2])\n",
    "\n",
    "    # Apply median filter to V channel\n",
    "    hsv[:, :, 2] = cv2.medianBlur(hsv[:, :, 2], 5)\n",
    "\n",
    "    # Convert back to RGB\n",
    "    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return img\n",
    "\n",
    "def ycbcr(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    # Convert to YCbCr\n",
    "    ycbcr = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # Split channels\n",
    "    y, cb, cr = cv2.split(ycbcr)\n",
    "\n",
    "    # Define rules\n",
    "    mask = (y >= 170) | (y < 145) & (cb <= 120) & (cb >= 50) & (cr > 120) & (cr < 220)\n",
    "\n",
    "    # Apply mask\n",
    "    img[mask] = 255\n",
    "\n",
    "    img[~mask] = 0\n",
    "\n",
    "    return img\n",
    "\n",
    "def rgb_space(image):\n",
    "    img = image.copy()\n",
    "\n",
    "    # Split channels\n",
    "    b, g, r = cv2.split(image)\n",
    "\n",
    "    # Define rules\n",
    "    rules = [\n",
    "        r > g,\n",
    "        g > b,\n",
    "        r > 190,\n",
    "        g > 90,\n",
    "        b < 140,\n",
    "        0.1 <= (g/(r+1)),\n",
    "        (g/(r+1)) <= 1,\n",
    "        0.1 <= (b/(r+1)),\n",
    "        (b/(r+1)) <= 0.85,\n",
    "        0.1 <= (b/(g+1)),\n",
    "        (b/(g+1)) <= 0.85\n",
    "    ]\n",
    "\n",
    "    # Apply rules\n",
    "    mask = np.all(rules, axis=0)\n",
    "\n",
    "    # Apply mask\n",
    "    img[mask] = 255\n",
    "\n",
    "    img[~mask] = 0\n",
    "\n",
    "    return img\n",
    "\n",
    "# Get percentage from 2 images spatially\n",
    "def get_percentage(img1, img2):\n",
    "    # Convert to grayscale\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Get percentage of white pixels location\n",
    "    percentage = np.sum(img1 == img2) / (img1.shape[0] * img1.shape[1])\n",
    "\n",
    "    return percentage\n",
    "\n",
    "def segment(img):\n",
    "    # Enhance image\n",
    "    low_light_img = low_light(img)\n",
    "\n",
    "    # Apply RGB space segmentation\n",
    "    rgb_space_img = rgb_space(img)\n",
    "    rgb_space_low_light = rgb_space(low_light_img)\n",
    "\n",
    "    # Apply YCbCr segmentation\n",
    "    ycbcr_img = ycbcr(img)\n",
    "    ycbcr_low_light = ycbcr(low_light_img)\n",
    "\n",
    "    # Combine RGB space and YCbCr segmented images using bitwise OR for both original and low light images\n",
    "    combined_img = cv2.bitwise_or(rgb_space_img, ycbcr_img) \n",
    "    combined_img_low = cv2.bitwise_or(rgb_space_low_light, ycbcr_low_light)\n",
    "\n",
    "    # Combine segmented original and low light images using bitwise AND\n",
    "    ultimate_combined = cv2.bitwise_and(combined_img, combined_img_low)\n",
    "\n",
    "    # Combine segmented RGB original and low light images using bitwise AND\n",
    "    rgb_combined = cv2.bitwise_or(rgb_space_img, rgb_space_low_light)\n",
    "\n",
    "    # Filter the result based on the threshold\n",
    "    output = np.zeros_like(ultimate_combined)\n",
    "\n",
    "    # Get the percentage of white pixels spatially\n",
    "    percentage = get_percentage(ultimate_combined, rgb_combined)\n",
    "\n",
    "    if percentage >= 0.75:\n",
    "        output = rgb_combined\n",
    "    else:\n",
    "        output = ultimate_combined\n",
    "\n",
    "    # Convert to grayscale\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Filling holes in the segmented image\n",
    "def fill_holes(image):\n",
    "    # Perform morphological closing operation to fill small holes\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "\n",
    "    # Perform closing operation\n",
    "    closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Perform dilation operation\n",
    "    filled_image = cv2.dilate(closed_image, kernel, iterations=2)\n",
    "\n",
    "    return filled_image\n",
    "\n",
    "# Crop the image from the original image\n",
    "def crop_image(image, x1, y1, x2, y2):\n",
    "    return image[y1:y2, x1:x2]\n",
    "\n",
    "def detect(image_path):\n",
    "    # Create output folder if not exists\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "\n",
    "    # Define coordinates\n",
    "    coordinates = []\n",
    "\n",
    "    ## YOLOv8\n",
    "    # Detecting objects in an image with YOLOv8\n",
    "    for model in modelv8:\n",
    "        results = model.predict(image_path)\n",
    "\n",
    "        # YOLOv8 coordinates\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                # print(box)\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                # print(x1, y1, x2, y2)\n",
    "                coordinates.append([x1, y1, x2, y2])\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    cv2.imwrite('./output/original.jpg', image)\n",
    "\n",
    "    # Initialize the binary image\n",
    "    binary_image = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "    # Iterate through the coordinates\n",
    "    for coordinate in coordinates:\n",
    "        # Get coordinates\n",
    "        x1, y1, x2, y2 = coordinate\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_image = crop_image(image, x1, y1, x2, y2)\n",
    "\n",
    "        # Segment the cropped image\n",
    "        segmented_image = segment(cropped_image)\n",
    "\n",
    "        # Fill holes in the segmented image\n",
    "        filled_image = fill_holes(segmented_image)\n",
    "\n",
    "        # Add the segmented image to the binary image\n",
    "        binary_image[y1:y2, x1:x2] = binary_image[y1:y2, x1:x2] + filled_image\n",
    "\n",
    "    cv2.imwrite('./output/binary.jpg', binary_image)\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/widss/tensorflow-test/Efficient-Wildfire-Detection-Framework-Based-on-AI-Using-CNN-and-Multi-Color-Filtering-main/FireSegmentationDataset/Video01/Video01_Frame/Video01_Frame001.jpg: 416x640 (no detections), 80.8ms\n",
      "Speed: 3.5ms preprocess, 80.8ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 /Users/widss/tensorflow-test/Efficient-Wildfire-Detection-Framework-Based-on-AI-Using-CNN-and-Multi-Color-Filtering-main/FireSegmentationDataset/Video01/Video01_Frame/Video01_Frame001.jpg: 192x256 2 fires, 14.6ms\n",
      "Speed: 0.4ms preprocess, 14.6ms inference, 0.3ms postprocess per image at shape (1, 3, 192, 256)\n",
      "CPU times: user 169 ms, sys: 54.1 ms, total: 223 ms\n",
      "Wall time: 115 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_20129/1968910619.py:48: RuntimeWarning: divide by zero encountered in divide\n",
      "  0.1 <= (g/(r+1)),\n",
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_20129/1968910619.py:49: RuntimeWarning: divide by zero encountered in divide\n",
      "  (g/(r+1)) <= 1,\n",
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_20129/1968910619.py:50: RuntimeWarning: divide by zero encountered in divide\n",
      "  0.1 <= (b/(r+1)),\n",
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_20129/1968910619.py:51: RuntimeWarning: divide by zero encountered in divide\n",
      "  (b/(r+1)) <= 0.85,\n",
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_20129/1968910619.py:52: RuntimeWarning: divide by zero encountered in divide\n",
      "  0.1 <= (b/(g+1)),\n",
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_20129/1968910619.py:53: RuntimeWarning: divide by zero encountered in divide\n",
      "  (b/(g+1)) <= 0.85\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "binary_image = detect('./FireSegmentationDataset/Video01/Video01_Frame/Video01_Frame001.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv bani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fire-segmentation'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 27 (delta 4), reused 5 (delta 1), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (27/27), 43.13 MiB | 8.03 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "warning: unable to access '/Users/widss/.config/git/attributes': Permission denied\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/whyspaceee/fire-segmentation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_advanced_segmentation_models as tasm\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "  BATCH_SIZE = 16\n",
    "  BUFFER_SIZE = 1000\n",
    "  N_CLASSES = 2\n",
    "  HEIGHT = 224\n",
    "  WIDTH = 224\n",
    "  BACKBONE_NAME = \"efficientnetb3\"\n",
    "  WEIGHTS = \"imagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model, layers, layer_names = tasm.create_base_model(name=BACKBONE_NAME, weights=WEIGHTS, height=HEIGHT, width=WIDTH, include_top=False, pooling=None)\n",
    "\n",
    "BACKBONE_TRAINABLE = False \n",
    "model = tasm.DeepLabV3plus(n_classes=N_CLASSES, base_model=base_model, output_layers=layers, backbone_trainable=BACKBONE_TRAINABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2dba181f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./fire-segmentation/weights/DeepLab224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = tf.keras.models.load_model('./fire-segmentation/detection-model-mobilenet224-v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    pred_mask = tf.cast(pred_mask, tf.float32)\n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "\n",
    "        # Convert mask to grayscale\n",
    "        if i == 1 or i == 2:\n",
    "            mask = np.array(display_list[i])\n",
    "            mask = np.squeeze(mask, axis=-1)\n",
    "            plt.imshow(mask, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(display_list[i])\n",
    "\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Convert image from RGB to HSV color space\n",
    "    hsv_image = cv2.cvtColor(reshaped_image, cv2.COLOR_RGB2HSV)\n",
    "    # Apply histogram equalization to the V channel of the HSV image\n",
    "    hsv_image[:, :, 2] = cv2.equalizeHist(hsv_image[:, :, 2])\n",
    "    # Perform median filtering on the V channel\n",
    "    filtered_image = cv2.medianBlur(hsv_image[:, :, 2], 5)\n",
    "    # Convert the HSV image back to RGB color space\n",
    "    processed_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "    return processed_image, filtered_image\n",
    "\n",
    "\n",
    "def extract_flames_rgb(image):\n",
    "    # Extract flames based on pixel distribution rules in RGB color space\n",
    "    red_channel = image[:, :, 0]\n",
    "    green_channel = image[:, :, 1]\n",
    "    flames_rgb = np.logical_and(red_channel > green_channel, red_channel > 177)\n",
    "    return flames_rgb\n",
    "\n",
    "\n",
    "def extract_flames_ycbcr(image):\n",
    "    # Convert RGB image to YCbCr color space\n",
    "    ycbcr_image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    # Extract flames based on pixel distribution rules in YCbCr color space\n",
    "    y_channel = ycbcr_image[:, :, 0]\n",
    "    cb_channel = ycbcr_image[:, :, 1]\n",
    "    cr_channel = ycbcr_image[:, :, 2]\n",
    "    cb_threshold = np.percentile(cb_channel, 45)\n",
    "    cr_threshold = np.percentile(cr_channel, 70)\n",
    "    flames_ycbcr = np.logical_and.reduce([\n",
    "        y_channel > 75,\n",
    "        cb_channel < cb_threshold,\n",
    "        cr_channel > cr_threshold\n",
    "    ])\n",
    "    return flames_ycbcr\n",
    "\n",
    "\n",
    "def apply_and_operator(flames_rgb, flames_ycbcr):\n",
    "    # Apply 'and' operator to obtain a complete flame image\n",
    "    flames = np.logical_and(flames_rgb, flames_ycbcr)\n",
    "    return flames\n",
    "\n",
    "\n",
    "def apply_or_operator(flames_rgb, flames_ycbcr):\n",
    "    # Apply 'and' operator to obtain a complete flame image\n",
    "    flames = np.logical_or(flames_rgb, flames_ycbcr)\n",
    "    return flames\n",
    "\n",
    "\n",
    "# Load the original flame image\n",
    "def segment_img(img):\n",
    "    mask = create_mask(\n",
    "        model.predict(np.asarray(img)[tf.newaxis, ...])\n",
    "    )\n",
    "    THRESHOLD = 0.8\n",
    "    pred = detector.predict(np.asarray(img)[tf.newaxis, ...])\n",
    "    if pred[0][0] > THRESHOLD:\n",
    "        return np.zeros((224, 224, 1))\n",
    "    processed_image, filtered_image = preprocess_image(np.asarray(img))\n",
    "    flames_rgb = extract_flames_rgb(processed_image)\n",
    "    flames_ycbcr = extract_flames_ycbcr(processed_image)\n",
    "    flames = apply_and_operator(flames_rgb, flames_ycbcr)\n",
    "    flamesImg = tf.keras.preprocessing.image.array_to_img(\n",
    "        (flames.astype(np.uint8) * 255)[:, :, np.newaxis])\n",
    "    \n",
    "    filtered = (flames.astype(np.uint8) * 255)[:, :, np.newaxis]\n",
    "    seg = apply_and_operator(filtered, mask)\n",
    "    return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "CPU times: user 299 ms, sys: 94.2 ms, total: 394 ms\n",
      "Wall time: 453 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[          0],\n",
       "        [          0],\n",
       "        [          0],\n",
       "        ...,\n",
       "        [          0],\n",
       "        [          0],\n",
       "        [          0]],\n",
       "\n",
       "       [[          0],\n",
       "        [          0],\n",
       "        [          0],\n",
       "        ...,\n",
       "        [          0],\n",
       "        [          0],\n",
       "        [          0]],\n",
       "\n",
       "       [[          0],\n",
       "        [          0],\n",
       "        [          0],\n",
       "        ...,\n",
       "        [          0],\n",
       "        [          0],\n",
       "        [          0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[          0],\n",
       "        [          0],\n",
       "        [          0],\n",
       "        ...,\n",
       "        [          0],\n",
       "        [          0],\n",
       "        [          0]],\n",
       "\n",
       "       [[          0],\n",
       "        [          0],\n",
       "        [          0],\n",
       "        ...,\n",
       "        [          0],\n",
       "        [          0],\n",
       "        [          0]],\n",
       "\n",
       "       [[          0],\n",
       "        [          0],\n",
       "        [          0],\n",
       "        ...,\n",
       "        [          0],\n",
       "        [          0],\n",
       "        [          0]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "segment_img(test_image_reshaped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3large (Functiona  (None, 7, 7, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 960)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                61504     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,058,913\n",
      "Trainable params: 62,561\n",
      "Non-trainable params: 2,996,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "detector.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
